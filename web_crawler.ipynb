{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H1>Web Crawler<H1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the urls in the main page we should:<br>\n",
    "* Check with request that we can read the webpage.\n",
    "* Check for  errors.\n",
    "* Check that it has more then 10 following links.\n",
    "* Colect the data.\n",
    "* Export to excel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test use case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In order to find the links we should find the \"< a >\" in the page using BeautifulSoup library. <br>\n",
    "* Then i'll check that it has more then 10 href in it and that they have a name.\n",
    "* find the text data and expot it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_site = 'https://www.ynet.co.il/home/0,7340,L-8,00.html'\n",
    "#test_site = 'https://crawler-test.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetch the webpage\n",
    "response = requests.get(test_site)\n",
    "response.raise_for_status()\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all <a> tags and get their href attributes\n",
    "links = soup.find_all('a', href=True)  # Filter out tags without href attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    urls.append(href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.ynet.co.il/news/category/184'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "name_lst = []\n",
    "text_list = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls[:20]:\n",
    "    try:\n",
    "        # Fetch the webpage\n",
    "        response = requests.get(i)\n",
    "        response.raise_for_status()\n",
    "        if href:\n",
    "        # Join the base URL with the relative URL\n",
    "            full_url = urljoin(i, href)\n",
    "            urls.append(full_url)\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        #print(soup)\n",
    "        soup.find_all()\n",
    "        # Safely get the title\n",
    "        if soup.title and soup.title.string:\n",
    "            name = soup.title.string\n",
    "        else:\n",
    "            name = 'No title found'\n",
    "\n",
    "        #print(name)\n",
    "        # Extract the text (or continue with other processing)\n",
    "        page_text = soup.get_text()  \n",
    "        #print(page_text)     \n",
    "     \n",
    "        url_list.append(i)\n",
    "        name_lst.append(name)\n",
    "        text_list.append(page_text)\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'URL': url_list,\n",
    "    'name':name_lst,\n",
    "    'Text': text_list\n",
    "})\n",
    "#print(df)\n",
    "# Save the DataFrame to an Excel file\n",
    "df.to_excel('urls_and_texts_test.xlsx', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genral Case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that I limit the max href to prevent excesive runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_urls_in_page(main_url):\n",
    "    try:\n",
    "        # Fetch the main page\n",
    "        print(f\"Processing URL: {main_url}\")\n",
    "\n",
    "        response = requests.get(main_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all valid links\n",
    "        links = soup.find_all('a', href=True)\n",
    "        urls = [urljoin(main_url, link['href']) for link in links if link['href'].startswith(('http', '/'))]\n",
    "        \n",
    "        if len(urls) < 10:\n",
    "            print(f\"Less than 10 links found on {main_url}. Skipping.\")\n",
    "            return\n",
    "        \n",
    "        url_list = []\n",
    "        name_list = []\n",
    "        text_list = []\n",
    "        \n",
    "        for url in urls[:50]:  # Limiting to 50 URLs for processing\n",
    "            try:\n",
    "                \n",
    "                # Fetch the webpage\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # Parse the HTML\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                \n",
    "                # Safely get the title\n",
    "                if soup.title and soup.title.string:\n",
    "                    name = soup.title.string\n",
    "                else:\n",
    "                    name = 'No title found'\n",
    "                \n",
    "                # Extract the text content of the page\n",
    "                page_text = soup.get_text()\n",
    "                \n",
    "                # Store the data\n",
    "                url_list.append(url)\n",
    "                name_list.append(name)\n",
    "                text_list.append(page_text)\n",
    "                \n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Error processing {url}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame({\n",
    "            'URL': url_list,\n",
    "            'Name': name_list,\n",
    "            'Text': text_list\n",
    "        })\n",
    "        \n",
    "        # Save the DataFrame to an Excel file\n",
    "        df.to_excel('href_name_text_general.xlsx', index=False)\n",
    "        print(\"Data has been saved to href_name_text_general.xlsx\")\n",
    "        return df\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching the main page {main_url}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_urls_in_page('https://www.ynet.co.il/home/0,7340,L-8,00.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36074a99c61e47048b0aab5181358f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='URLs:', layout=Layout(height='100px', width='500px'), placeholder='Paste your â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225c7df1bd1a4257b5226392e7b95e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Process URL', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing URL: https://www.ynet.co.il/home/0,7340,L-8,00.html\n",
      "Data has been saved to href_name_text_general.xlsx\n"
     ]
    }
   ],
   "source": [
    "def on_button_click(b):  # Accepts a parameter 'b', which is passed by the button widget\n",
    "    url = text_widget.value\n",
    "    if url:\n",
    "\n",
    "        # Process the URLs\n",
    "        get_urls_in_page(url)\n",
    "        \n",
    "# Widget for URL input\n",
    "text_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Paste your URLs here, one per line',\n",
    "    description='URLs:',\n",
    "    disabled=False,\n",
    "    layout=widgets.Layout(width='500px', height='100px')\n",
    ")\n",
    "\n",
    "# Button to trigger processing\n",
    "button = widgets.Button(description=\"Process URL\")\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# Display the widgets\n",
    "display(text_widget, button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
